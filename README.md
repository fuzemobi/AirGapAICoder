# AirGapAICoder

> **Enterprise-grade AI coding assistant for air-gapped environments**

A complete solution for running powerful AI-assisted coding tools in secure, offline environments using open-source models and local GPU acceleration.

![Version](https://img.shields.io/badge/version-1.0.2-blue.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Platform](https://img.shields.io/badge/platform-Windows%2011-blue.svg)
![GPU](https://img.shields.io/badge/GPU-NVIDIA-green.svg)

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Quickstart Guide](#quickstart-guide)
- [System Requirements](#system-requirements)
- [Deployment Process](#deployment-process)
- [Documentation](#documentation)
- [Architecture](#architecture)
- [Contributing](#contributing)
- [License](#license)
- [Author](#author)

## ğŸ¯ Overview

AirGapAICoder enables organizations to deploy AI-powered coding assistants in completely air-gapped (offline) environments. By leveraging open-source large language models running on local GPU hardware, it provides enterprise-grade coding assistance without any external dependencies or cloud connectivity.

**Perfect for:**
- ğŸ”’ Secure environments requiring complete network isolation
- ğŸ¢ Organizations with strict data sovereignty requirements
- ğŸ’¼ Enterprise development teams in regulated industries
- ğŸ›¡ï¸ Government and defense contractors
- ğŸ” Companies handling sensitive intellectual property

## âœ¨ Features

### Core Capabilities

- âœ… **Fully Offline Operation** - Zero internet dependency after initial setup
- âœ… **GPU-Accelerated Inference** - High-performance local AI model execution
- âœ… **Extended Context Windows** - 131k tokens for large codebase analysis
- âœ… **Multi-User Support** - Network-accessible server for team collaboration
- âœ… **Universal CLI Access** - Use from any terminal on any platform (no IDE required)
- âœ… **Optional IDE Integration** - VS Code + Cline extension available
- âœ… **Multiple Models** - Switch between coding and reasoning-focused models
- âœ… **Enterprise Security** - Complete data residency and audit logging
- âœ… **Easy Deployment** - Automated installation scripts and packaging

### AI Models Included

| Model | Parameters | VRAM | Context | Best For |
|-------|-----------|------|---------|----------|
| **Qwen 2.5 Coder 32B** | 32 Billion | 24GB | 131k tokens | General coding, refactoring |
| **DeepSeek R1 32B** | 32 Billion | 24GB | 131k tokens | Complex reasoning, algorithms |
| **Qwen 2.5 Coder 14B** | 14 Billion | 12GB | 131k tokens | Lightweight, faster responses |

## ğŸš€ Quickstart Guide

### From Zero to AI Coding in 5 Steps

**Prerequisites:** A server with NVIDIA GPU and one internet-connected computer for preparation.

#### Step 1: Prepare Package (Internet-Connected System)

```bash
# Clone repository
git clone https://github.com/fuzemobi/AirGapAICoder.git
cd AirGapAICoder

# Run preparation script (downloads Ollama + models)
cd scripts/preparation
./pull-all.sh

# For production models (47GB), first run:
export PULL_PRODUCTION_MODELS=true
./pull-all.sh

# Package is created at ~/airgap-package
```

**Windows (PowerShell):**
```powershell
git clone https://github.com/fuzemobi/AirGapAICoder.git
cd AirGapAICoder\scripts\preparation
.\pull-all.ps1
```

#### Step 2: Transfer to Air-Gap Server

```bash
# Copy to USB drive
cp -r ~/airgap-package /Volumes/USB/

# On server, copy from USB
cp -r /media/USB/airgap-package ~/
```

#### Step 3: Install on Server

**Windows Server:**
```powershell
cd scripts\installation\server
.\install-windows.ps1 C:\airgap-package
```

**Ubuntu Server:**
```bash
cd scripts/installation/server
sudo ./install-ubuntu.sh ~/airgap-package
```

**macOS (Testing):**
```bash
cd scripts/installation/server
./install-macos.sh ~/airgap-package
```

#### Step 4: Verify Server

```bash
# Check server health
curl http://localhost:11434/api/tags

# Or from another machine on network
curl http://SERVER_IP:11434/api/tags

# List models
ollama list
```

#### Step 5: Start Using AI!

**From ANY Terminal:**

```bash
# Using CLI wrapper
./scripts/cli/ollama-cli.sh run SERVER_IP:11434 qwen-32b-cline \
  "Write a Python function to calculate prime numbers"

# Using curl directly
curl http://SERVER_IP:11434/api/generate -d '{
  "model": "qwen-32b-cline",
  "prompt": "Write a REST API endpoint in Python FastAPI",
  "stream": false
}' | jq -r '.response'

# Using PowerShell
Invoke-RestMethod http://SERVER_IP:11434/api/generate -Method POST -Body '{
  "model": "qwen-32b-cline",
  "prompt": "Create a React component for user login",
  "stream": false
}' -ContentType "application/json" | Select -ExpandProperty response
```

**From Python:**
```python
import requests

response = requests.post('http://SERVER_IP:11434/api/generate', json={
    'model': 'qwen-32b-cline',
    'prompt': 'Write a Python function to validate email addresses',
    'stream': False
})
print(response.json()['response'])
```

**Optional: VS Code + Cline**

If you prefer a GUI:
1. Install VS Code
2. Install Cline extension from package
3. Configure: Settings â†’ Base URL: `http://SERVER_IP:11434`
4. Start chatting in Cline sidebar

### Quick Examples

**Code Generation:**
```bash
# Generate a complete Python module
./scripts/cli/ollama-cli.sh run SERVER:11434 qwen-32b-cline \
  "Create a Python class for managing a shopping cart with add, remove, checkout methods"

# Generate tests
./scripts/cli/ollama-cli.sh run SERVER:11434 qwen-32b-cline \
  "Write pytest tests for this function: $(cat mycode.py)"
```

**Code Review:**
```bash
# Review code for issues
curl http://SERVER:11434/api/generate -d "{
  \"model\": \"qwen-32b-cline\",
  \"prompt\": \"Review this code for bugs and security issues: $(cat app.py)\",
  \"stream\": false
}" | jq -r '.response'
```

**Refactoring:**
```bash
# Improve code quality
./scripts/cli/ollama-cli.sh run SERVER:11434 deepseek-r1-32b-cline \
  "Refactor this code to use async/await and add error handling: $(cat sync_code.py)"
```

**Documentation:**
```bash
# Generate docstrings
./scripts/cli/ollama-cli.sh run SERVER:11434 qwen-32b-cline \
  "Add comprehensive docstrings to: $(cat module.py)" > documented_module.py
```

### That's It!

You now have a fully functional, air-gapped AI coding assistant accessible from any terminal, any platform, anywhere on your network.

**No cloud required. No subscriptions. No data leakage. Complete control.**

---

## ğŸ’» System Requirements

### Server (AI Inference Host)

| Component | Minimum | Recommended | Optimal |
|-----------|---------|-------------|---------|
| **GPU** | NVIDIA RTX 3090 (24GB) | RTX 4090 (24GB) | A6000 (48GB) |
| **RAM** | 32GB DDR4 | 64GB DDR5 | 128GB DDR5 |
| **Storage** | 500GB NVMe SSD | 1TB NVMe SSD | 2TB NVMe SSD |
| **CPU** | 8-core x64 | 12-core x64 | 16+ core x64 |
| **OS** | Windows 11 Pro | Windows 11 Pro | Windows 11 Enterprise |
| **Network** | 1 Gbps Ethernet | 2.5 Gbps | 10 Gbps |

### Client Workstations

- **OS**: Windows 10/11, macOS, or Linux
- **RAM**: 8GB minimum, 16GB recommended
- **Storage**: 10GB free space
- **Network**: Connected to same local network as server

### Software Components

- **Ollama** - LLM inference engine
- **VS Code** - IDE
- **Cline Extension** - AI assistant interface
- **NVIDIA Driver** - GPU acceleration
- **CUDA Toolkit** - GPU computing platform

## ğŸ“¦ Deployment Process

The full deployment follows three phases:

1. **Preparation** (Internet-connected) - Download Ollama, models, and dependencies
2. **Transfer** (USB/removable media) - Move ~50GB package to air-gap server
3. **Installation** (Air-gapped server) - Automated setup with scripts

See [Quickstart Guide](#quickstart-guide) above for step-by-step instructions.

**Detailed guides available:**
- [SERVER-SETUP.md](docs/SERVER-SETUP.md) - Complete server deployment
- [INSTALLATION.md](docs/INSTALLATION.md) - Detailed installation procedures
- [CLI-USAGE.md](docs/CLI-USAGE.md) - Terminal usage examples

## ğŸ“š Documentation

Comprehensive documentation is available in the `docs/` directory:

### Getting Started

- **[CLI Usage Guide](docs/CLI-USAGE.md)** - Use from any terminal (recommended)
- **[Client Usage Guide](docs/CLIENT-USAGE.md)** - VS Code + Cline setup (optional)
- **[Server Setup Guide](docs/SERVER-SETUP.md)** - Complete deployment guide

### Advanced

- **[Installation Guide](docs/INSTALLATION.md)** - Detailed installation procedures
- **[Operations Guide](docs/OPERATIONS.md)** - Day-to-day operation and maintenance
- **[Architecture](docs/ARCHITECTURE.md)** - Technical system design
- **[Requirements](docs/REQUIREMENTS.md)** - Detailed requirements specification

### For Developers

- **[CLAUDE.md](CLAUDE.md)** - Claude Code development guidance

### Quick Reference

**CLI Usage (from any terminal):**

```bash
# Check server status
./scripts/cli/ollama-cli.sh status SERVER_IP:11434

# List models
./scripts/cli/ollama-cli.sh models SERVER_IP:11434

# Generate code
./scripts/cli/ollama-cli.sh run SERVER_IP:11434 qwen-32b-cline "Write a Python function"

# Or use HTTP API directly
curl http://SERVER_IP:11434/api/generate -d '{
  "model": "qwen-32b-cline",
  "prompt": "Write a hello world",
  "stream": false
}'
```

**Server Management:**

```powershell
# Server commands
ollama serve                          # Start server
ollama list                           # List models
ollama ps                             # Check running models
nvidia-smi                            # Monitor GPU

# Monitoring
nvidia-smi -l 1                       # Real-time GPU stats
Get-Process ollama | Format-List *   # Process details
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OMEN 35L Server (Windows 11)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Ollama Server :11434          â”‚     â”‚
â”‚  â”‚  â”œâ”€ Qwen 2.5 Coder 32B         â”‚     â”‚
â”‚  â”‚  â”œâ”€ DeepSeek R1 32B            â”‚     â”‚
â”‚  â”‚  â””â”€ Qwen 2.5 Coder 14B         â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚           â†• NVIDIA GPU                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Local Network
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Client  â”‚       â”‚ Client  â”‚
    â”‚ VS Code â”‚       â”‚ VS Code â”‚
    â”‚ + Cline â”‚       â”‚ + Cline â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

1. **Inference Layer**: Ollama server with GPU-accelerated LLM inference
2. **Network Layer**: HTTP API for client-server communication (port 11434)
3. **Client Layer**: VS Code with Cline extension for developer interaction

**Technology Stack:**
- **Inference Engine**: Ollama (open-source)
- **AI Models**: Qwen 2.5 Coder, DeepSeek R1 (open-source)
- **Interface**: HTTP REST API (universal access)
- **CLI**: Cross-platform command-line tools
- **Optional Client**: VS Code + Cline extension
- **GPU**: NVIDIA with CUDA acceleration
- **Platform**: Windows Server 2022, Ubuntu 22.04+, or Windows 11

## ğŸ” Security Features

- âœ… **Network Isolation**: No outbound internet connectivity required
- âœ… **Data Residency**: All processing happens locally
- âœ… **Access Control**: IP-based client whitelisting
- âœ… **Audit Logging**: Comprehensive activity logs
- âœ… **Firewall Protection**: Strict port restrictions
- âœ… **No External APIs**: Zero dependency on cloud services

## ğŸ¯ Use Cases

### Software Development

- Code generation from any terminal
- Refactoring and optimization
- Bug fixing and debugging
- Documentation generation
- Code review assistance
- Integration with any text editor or IDE
- Shell script automation
- CI/CD pipeline integration

### Enterprise Applications

- Secure development environments
- Regulated industry compliance (finance, healthcare, defense)
- Intellectual property protection
- Custom model fine-tuning (future)
- Team collaboration without data leakage

## ğŸ“Š Performance

With recommended hardware (RTX 4090, 64GB RAM):

| Metric | Performance |
|--------|-------------|
| **Time to First Token** | < 500ms |
| **Generation Speed** | 50+ tokens/second |
| **Concurrent Users** | 3-5 simultaneous |
| **Context Window** | 131,072 tokens |
| **Model Load Time** | < 10 seconds |
| **GPU Utilization** | 80-100% during inference |

## ğŸ› ï¸ Troubleshooting

### Common Issues

**Server not responding:**
```powershell
# Restart Ollama
Stop-Process -Name "ollama" -Force
Start-Process "ollama" -ArgumentList "serve" -WindowStyle Hidden
```

**GPU not detected:**
```powershell
# Verify NVIDIA driver
nvidia-smi

# Check CUDA installation
nvcc --version
```

**Clients can't connect:**
```powershell
# Check firewall
Get-NetFirewallRule -DisplayName "Ollama"

# Verify server is listening
netstat -an | Select-String "11434"
```

See [Operations Guide](docs/OPERATIONS.md) for complete troubleshooting.

## ğŸ—ºï¸ Roadmap

### v1.0 - Core Platform (Current) âœ…
- âœ… Air-gap deployment automation
- âœ… Multi-platform support (Windows Server, Ubuntu, macOS)
- âœ… Universal CLI access from any terminal
- âœ… GPU-accelerated inference
- âœ… Multi-user network support
- âœ… Extended 131k token context windows
- âœ… Service monitoring and auto-restart
- âœ… Comprehensive documentation

### v1.1 - Containerization (Q1 2026)
- ğŸ”œ Podman-based deployment
- ğŸ”œ Pre-built container images
- ğŸ”œ Windows Server container support
- ğŸ”œ AMD Ryzen 7 optimization
- ğŸ”œ Simplified updates via containers
- ğŸ”œ Container orchestration scripts

### v1.2 - Enhanced Management (Q2 2026)
- ğŸ”œ Web-based admin dashboard
- ğŸ”œ Real-time metrics and analytics
- ğŸ”œ Advanced monitoring alerts
- ğŸ”œ Performance profiling tools
- ğŸ”œ User activity tracking

### v2.0 - Enterprise Features (Q3-Q4 2026)
- ğŸ”œ Multi-GPU support
- ğŸ”œ Load balancing across servers
- ğŸ”œ RAG (Retrieval-Augmented Generation)
- ğŸ”œ Custom model fine-tuning
- ğŸ”œ RBAC (Role-Based Access Control)
- ğŸ”œ TLS/SSL encryption
- ğŸ”œ Advanced audit logging

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### How to Contribute

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup

```powershell
# Clone repository
git clone https://github.com/fuzemobi/AirGapAICoder.git
cd AirGapAICoder

# Follow installation guide
# See docs/INSTALLATION.md
```

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Fuzemobi, LLC**
**Chad Rosenbohm**

## ğŸ™ Acknowledgments

- **Ollama** - Open-source LLM inference engine
- **Qwen Team** - Qwen 2.5 Coder models
- **DeepSeek** - DeepSeek R1 reasoning model
- **Cline Extension** - VS Code AI assistant interface
- **NVIDIA** - CUDA and GPU technology

## ğŸ“ Support

For issues, questions, or suggestions:

- ğŸ“– Check the [documentation](docs/)
- ğŸ› Open an [issue](https://github.com/fuzemobi/AirGapAICoder/issues)
- ğŸ’¬ Start a [discussion](https://github.com/fuzemobi/AirGapAICoder/discussions)

## âš ï¸ Disclaimer

This solution is designed for legal, defensive security purposes only. Users are responsible for ensuring compliance with all applicable licenses, regulations, and organizational policies.

---

---

## ğŸ¯ Project Status

**Current Version:** 1.0.2
**Status:** Production Ready
**Last Updated:** 2025-10-19

### Recent Updates

- âœ… Universal CLI access from any terminal
- âœ… Comprehensive quickstart guide
- âœ… Multi-platform installation scripts
- âœ… Service monitoring and auto-restart
- âœ… Complete documentation suite

### What's Working

- **Preparation Scripts**: Download and package components for air-gap
- **Installation Scripts**: Automated setup for Windows, Ubuntu, macOS
- **CLI Tools**: Remote management from any terminal
- **Monitoring**: Health checks with automatic restart
- **Documentation**: Complete guides for deployment and usage

### Tested Platforms

- âœ… macOS (development and testing)
- âš™ï¸ Windows Server 2022 (scripts ready, pending hardware testing)
- âš™ï¸ Ubuntu Server 22.04+ (scripts ready, pending hardware testing)

### Community

- **GitHub Repository**: https://github.com/fuzemobi/AirGapAICoder
- **Issues**: Report bugs or request features
- **Discussions**: Share your deployment experiences
- **Contributions**: Pull requests welcome!

---

**Made with â¤ï¸ for secure, offline AI-assisted development**

**Star â­ this repo if you find it useful!**
